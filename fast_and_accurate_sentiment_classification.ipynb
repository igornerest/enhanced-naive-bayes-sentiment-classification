{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de textos (TRAIN):\n",
      "25000\n",
      "Total de textos (TEST):\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "PATHS = {\n",
    "    'TRAIN' : {\n",
    "        'POSITIVE' : './IMDB_dataset/train/pos',\n",
    "        'NEGATIVE' : './IMDB_dataset/train/neg'\n",
    "    },\n",
    "    'TEST' : {\n",
    "        'POSITIVE' : './IMDB_dataset/test/pos',\n",
    "        'NEGATIVE' : './IMDB_dataset/test/neg'\n",
    "    }\n",
    "}\n",
    "\n",
    "documents_amount = {\n",
    "    'TRAIN' : {\n",
    "        'POSITIVE' : len(os.listdir(PATHS['TRAIN']['POSITIVE'])),\n",
    "        'NEGATIVE' : len(os.listdir(PATHS['TRAIN']['NEGATIVE']))\n",
    "    }, \n",
    "    'TEST' : {\n",
    "        'POSITIVE' : len(os.listdir(PATHS['TEST']['POSITIVE'])),\n",
    "        'NEGATIVE' : len(os.listdir(PATHS['TEST']['NEGATIVE']))\n",
    "    }\n",
    "}\n",
    "\n",
    "total_documents = {\n",
    "    'TRAIN' : sum(documents_amount['TRAIN'].values()),\n",
    "    'TEST'  : sum(documents_amount['TEST'].values())\n",
    "}\n",
    "\n",
    "classes = {\n",
    "    'POSITIVE' : {\n",
    "        'docs' : {},\n",
    "        'WORDS' : [],\n",
    "        'FREQUENCY' : {},\n",
    "        'PRIOR': 0,\n",
    "        'NGRAMS': {\n",
    "            '1': dict([]),\n",
    "            '2': dict([]),\n",
    "            '3': dict([])\n",
    "        }\n",
    "    },\n",
    "    'NEGATIVE' : {\n",
    "        'docs' : {},\n",
    "        'WORDS' : [],\n",
    "        'FREQUENCY' : {},\n",
    "        'PRIOR': 0,\n",
    "        'NGRAMS': {\n",
    "            '1': dict([]),\n",
    "            '2': dict([]),\n",
    "            '3': dict([])\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for tipo, total in total_documents.items():\n",
    "    print(f\"Total de textos ({tipo}):\\n{total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r\"[-'a-zA-ZÀ-ÖØ-öø-ÿ]+|[.,;!?]\"\n",
    "CORPUS_USE_PERCENTAGE = 0.01\n",
    "\n",
    "def get_document_words(document_path):\n",
    "    content = open(document_path, 'r', encoding=\"UTF-8\").read().lower()\n",
    "    return negation_handling(re.findall(regex, content))\n",
    "\n",
    "def get_document_words_ngram(document_path, classe):\n",
    "    content = open(document_path, 'r', encoding=\"UTF-8\").read().lower()\n",
    "    words = negation_handling_ngram(re.findall(regex, content))\n",
    "    \n",
    "    # unigrams\n",
    "    for w in words:\n",
    "        if w not in classe['NGRAMS']['1']:\n",
    "            classe['NGRAMS']['1'][w] = 0\n",
    "        classe['NGRAMS']['1'][w] += 1\n",
    "    \n",
    "    # bigrams\n",
    "    #for i in range(0, len(words)-1):\n",
    "    #    b = (words[i], words[i+1])\n",
    "    #    if b not in classe['NGRAMS']['2']:\n",
    "    #        classe['NGRAMS']['2'][b] = 0\n",
    "    #    classe['NGRAMS']['2'][b] += 1\n",
    "        \n",
    "    # trigrams\n",
    "    #for i in range(0, len(words)-2):\n",
    "    #    t = (words[i], words[i+1], words[i+2])\n",
    "    #    if t not in classe['NGRAMS']['3']:\n",
    "    #        classe['NGRAMS']['3'][t] = 0\n",
    "    #    classe['NGRAMS']['3'][t] += 1\n",
    "\n",
    "def update_words_list(path, words_list):\n",
    "    for i in range(0, int(len(os.listdir(path))*CORPUS_USE_PERCENTAGE)):\n",
    "        file_name = os.listdir(path)[i]\n",
    "        words     = get_document_words(path+\"/\"+file_name)\n",
    "        words_list.extend(words)\n",
    "        \n",
    "def update_words_list_ngram(path, words_list, classe):\n",
    "    for i in range(0, int(len(os.listdir(path))*CORPUS_USE_PERCENTAGE)):\n",
    "        file_name = os.listdir(path)[i]\n",
    "        get_document_words_ngram(path+\"/\"+file_name, classe)\n",
    "\n",
    "def update_frequency(frequency, words):\n",
    "    for word in set(words):\n",
    "        frequency[word] = words.count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negation Handling\n",
    "punctuationRe = r\"[,.;!?]\"\n",
    "negationRe = r\"not|no|\\w*n't\"\n",
    "\n",
    "def negation_handling(words):\n",
    "    negated = False\n",
    "    words_set = set()\n",
    "\n",
    "    for word in words:\n",
    "        if (re.fullmatch(punctuationRe, word)):\n",
    "            negated = False\n",
    "            continue\n",
    "        if (re.fullmatch(negationRe, word)):\n",
    "            negated = not negated\n",
    "            continue\n",
    "        if (negated):\n",
    "            word = \"not_\" + word\n",
    "        words_set.add(word)\n",
    "            \n",
    "    return words_set\n",
    "\n",
    "def negation_handling_ngram(words):\n",
    "    negated = False\n",
    "    words_list = list()\n",
    "\n",
    "    for word in words:\n",
    "        if (re.fullmatch(punctuationRe, word)):\n",
    "            negated = False\n",
    "            continue\n",
    "        if (re.fullmatch(negationRe, word)):\n",
    "            negated = not negated\n",
    "            continue\n",
    "        if (negated):\n",
    "            word = \"not_\" + word\n",
    "        words_list.append(word)\n",
    "            \n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAPLACIAN_SMOOTHING = 1\n",
    "\n",
    "def likelihood_word_class(word, classe):\n",
    "    count_word_class = 0\n",
    "    if (word in classes[classe]['FREQUENCY']):\n",
    "        count_word_class = classes[classe]['FREQUENCY'][word]\n",
    "    \n",
    "    total_words_class = len(classes[classe]['WORDS'])\n",
    "    \n",
    "    return (count_word_class + LAPLACIAN_SMOOTHING)/((LAPLACIAN_SMOOTHING + 1) * total_words_class)\n",
    "\n",
    "def likelihood_doc_class(doc_path, classe):                            \n",
    "    likelihood = 0\n",
    "\n",
    "    for word in get_document_words(doc_path):\n",
    "        likelihood +=  math.log(likelihood_word_class(word, classe))\n",
    "    \n",
    "    return likelihood + classes[classe]['PRIOR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train:\n",
    "#     average time @ core i7, 7th gen\n",
    "#     (x%  corpus: y min)\n",
    "#     10% : 03 min\n",
    "#     20% : 08 min\n",
    "#     25% : 13 min\n",
    "#     33% : 19 min\n",
    "#     50% :+40 min\n",
    "#    100% :  ? min\n",
    "#\n",
    "#     average time @ core i5, 5th gen\n",
    "#     (x%  corpus: y min)\n",
    "#     5% : 06 min\n",
    "\n",
    "for classe in classes:\n",
    "    classes[classe]['PRIOR'] = math.log(documents_amount['TRAIN'][classe]/total_documents['TRAIN'])\n",
    "    update_words_list(PATHS['TRAIN'][classe], classes[classe]['WORDS'])\n",
    "    update_frequency(classes[classe]['FREQUENCY'], classes[classe]['WORDS'])\n",
    "    #update_words_list_ngram(PATHS['TRAIN'][classe], classes[classe]['WORDS'], classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy (POSITIVE) = 70.016%\n",
      "\n",
      "Accuracy (NEGATIVE) = 78.808%\n",
      "\n",
      "Accuracy (average) = 74.412%\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "def test(classe, total_documents):\n",
    "    correct_tests = 0\n",
    "\n",
    "    path = PATHS['TEST'][classe]\n",
    "    for file_name in os.listdir(path):\n",
    "          likelihood = {}\n",
    "          \n",
    "          for classe_aux in classes:\n",
    "              likelihood[classe_aux] = likelihood_doc_class(path+\"/\"+file_name, classe_aux)\n",
    "          \n",
    "          if (classe == max(likelihood, key=likelihood.get)):\n",
    "              correct_tests += 1\n",
    "    \n",
    "    return 100*correct_tests/total_documents\n",
    "\n",
    "accuracy = {}\n",
    "\n",
    "for classe in classes:\n",
    "    accuracy[classe] = test(classe, documents_amount['TEST'][classe])\n",
    "    print(f\"\\nAccuracy ({classe}) = {accuracy[classe]}%\")\n",
    "\n",
    "print(f\"\\nAccuracy (average) = {sum(accuracy.values())/len(accuracy)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
